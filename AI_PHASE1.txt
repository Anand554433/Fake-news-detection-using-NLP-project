{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Earthquake Prediction\nIt is well known that if a disaster has happened in a region, it is likely to happen there again. Some regions really have frequent earthquakes, but this is just a comparative quantity compared to other regions.\nSo, predicting the earthquake with Date and Time, Latitude and Longitude from previous data is not a trend which follows like other things, it is natural occuring. ","metadata":{"_uuid":"d74c8a9da867939d1dcc94bcded9cfe04729377d"}},{"cell_type":"markdown","source":"Import the necessary libraries required for buidling the model and data analysis of the earthquakes.","metadata":{"_uuid":"1039e9219b8df39a5a2238ea87605930fd5755ee"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport os\nprint(os.listdir(\"../input\"))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Read the data from csv and also columns which are necessary for the model and the column which needs to be predicted.","metadata":{"_uuid":"839689014c0450052ccc8ac8f75766f25ba8c5ba"}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/database.csv\")\ndata.head()","metadata":{"_uuid":"0cf62f03d1e37f69de7358546abc41af384a8ff6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"_uuid":"393374ce2d7b88cfa5894078a5fcfd23cf999a0b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Figure out the main features from earthquake data and create a object of that features, namely, Date, Time, Latitude, Longitude, Depth, Magnitude.","metadata":{"_uuid":"6c4723dccc9ead17dd169cadd22433ba56b556e9"}},{"cell_type":"code","source":"data = data[['Date', 'Time', 'Latitude', 'Longitude', 'Depth', 'Magnitude']]\ndata.head()","metadata":{"_uuid":"ef3b8a3ff1da295b7578049bbbe87c8dffc24af5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, the data is random we need to scale according to inputs to the model. In this, we convert given Date and Time to Unix time which is in seconds and a numeral. This can be easily used as input for the network we built.","metadata":{"_uuid":"f35da0b2e5cc4062175ba73ecab96cf9a3527e0a"}},{"cell_type":"code","source":"import datetime\nimport time\n\ntimestamp = []\nfor d, t in zip(data['Date'], data['Time']):\n    try:\n        ts = datetime.datetime.strptime(d+' '+t, '%m/%d/%Y %H:%M:%S')\n        timestamp.append(time.mktime(ts.timetuple()))\n    except ValueError:\n        # print('ValueError')\n        timestamp.append('ValueError')","metadata":{"_uuid":"ab5cb17dbfc84f427e02d01b22aec587e8d20a5d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"timeStamp = pd.Series(timestamp)\ndata['Timestamp'] = timeStamp.values","metadata":{"_uuid":"51caed8126e4ee74094a42d11a64e2d613c50ae8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_data = data.drop(['Date', 'Time'], axis=1)\nfinal_data = final_data[final_data.Timestamp != 'ValueError']\nfinal_data.head()","metadata":{"_uuid":"e3e5a92d76a1fb95fe50e30706cf83fa73a41c67","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization\nHere, all the earthquakes from the database in visualized on to the world map which shows clear representation of the locations where frequency of the earthquake will be more. ","metadata":{"_uuid":"e0161f71675503ec0e51bea6b8f5c0276f7917f2"}},{"cell_type":"code","source":"from mpl_toolkits.basemap import Basemap\n\nm = Basemap(projection='mill',llcrnrlat=-80,urcrnrlat=80, llcrnrlon=-180,urcrnrlon=180,lat_ts=20,resolution='c')\n\nlongitudes = data[\"Longitude\"].tolist()\nlatitudes = data[\"Latitude\"].tolist()\n#m = Basemap(width=12000000,height=9000000,projection='lcc',\n            #resolution=None,lat_1=80.,lat_2=55,lat_0=80,lon_0=-107.)\nx,y = m(longitudes,latitudes)","metadata":{"_uuid":"229fc3316366e5e9d6c101084829406e94110d89","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,10))\nplt.title(\"All affected areas\")\nm.plot(x, y, \"o\", markersize = 2, color = 'blue')\nm.drawcoastlines()\nm.fillcontinents(color='coral',lake_color='aqua')\nm.drawmapboundary()\nm.drawcountries()\nplt.show()","metadata":{"_uuid":"cd38f9d1f7b894a912268c1e043a065803cb2631","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting the Data\nFirstly, split the data into Xs and ys which are input to the model and output of the model respectively. Here, inputs are TImestamp, Latitude and Longitude and outputs are Magnitude and Depth. Split the Xs and ys into train and test with validation. Training dataset contains 80% and Test dataset contains 20%.","metadata":{"_uuid":"9eb980f99372feccb267c86d468f36d5d6808c39"}},{"cell_type":"code","source":"X = final_data[['Timestamp', 'Latitude', 'Longitude']]\ny = final_data[['Magnitude', 'Depth']]","metadata":{"_uuid":"4f3e27f35bf788084db7eb98d5c55bddfa07cde2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cross_validation import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(X_train.shape, X_test.shape, y_train.shape, X_test.shape)","metadata":{"_uuid":"31d9ba582a7af902b59788a862be5d1e1523d639","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we used the RandomForestRegressor model to predict the outputs, we see the strange prediction from this with score above 80% which can be assumed to be best fit but not due to its predicted values.","metadata":{"_uuid":"3d5c659c096cadc28f2cdd65abfb7a61fc0777c8"}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nreg = RandomForestRegressor(random_state=42)\nreg.fit(X_train, y_train)\nreg.predict(X_test)","metadata":{"_uuid":"b6f9808731d449921637b96b9a2aef2961239a0d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg.score(X_test, y_test)","metadata":{"_uuid":"a2b8480ab26f551e16e45d3c70cda8192a402c9a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparameters = {'n_estimators':[10, 20, 50, 100, 200, 500]}\n\ngrid_obj = GridSearchCV(reg, parameters)\ngrid_fit = grid_obj.fit(X_train, y_train)\nbest_fit = grid_fit.best_estimator_\nbest_fit.predict(X_test)","metadata":{"_uuid":"682a29a27dbbaf14303a8881f7fc13aacbc5b309","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_fit.score(X_test, y_test)","metadata":{"_uuid":"35d6cf6059a072189e9f95dad28f5cc0d8b9a7db","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Neural Network model\nIn the above case it was more kind of linear regressor where the predicted values are not as expected. So, Now, we build the neural network to fit the data for training set. Neural Network consists of three Dense layer with each 16, 16, 2 nodes and relu, relu and softmax as activation function.","metadata":{"_uuid":"215d5185f6964cf0ffbf313d6d5e285e9ad873db"}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\n\ndef create_model(neurons, activation, optimizer, loss):\n    model = Sequential()\n    model.add(Dense(neurons, activation=activation, input_shape=(3,)))\n    model.add(Dense(neurons, activation=activation))\n    model.add(Dense(2, activation='softmax'))\n    \n    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n    \n    return model","metadata":{"_uuid":"ffcfb0d29010f5addab990783abab0923f7a1dcb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this, we define the hyperparameters with two or more options to find the best fit.","metadata":{"_uuid":"b053784767ceb4eb8171781f4667f4d296c4b197"}},{"cell_type":"code","source":"from keras.wrappers.scikit_learn import KerasClassifier\n\nmodel = KerasClassifier(build_fn=create_model, verbose=0)\n\n# neurons = [16, 64, 128, 256]\nneurons = [16]\n# batch_size = [10, 20, 50, 100]\nbatch_size = [10]\nepochs = [10]\n# activation = ['relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear', 'exponential']\nactivation = ['sigmoid', 'relu']\n# optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\noptimizer = ['SGD', 'Adadelta']\nloss = ['squared_hinge']\n\nparam_grid = dict(neurons=neurons, batch_size=batch_size, epochs=epochs, activation=activation, optimizer=optimizer, loss=loss)","metadata":{"_uuid":"37e01b437bf48b4542a1cde0f9ece0ee427d7d6d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we find the best fit of the above model and get the mean test score and standard deviation of the best fit model.","metadata":{"_uuid":"363f1a26772010cc10044357fedc2d8330cc7722"}},{"cell_type":"code","source":"grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\ngrid_result = grid.fit(X_train, y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","metadata":{"_uuid":"e95a9220f51b2483cb581502a00e8aef484ad6ee","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best fit parameters are used for same model to compute the score with training data and testing data.","metadata":{"_uuid":"af169ec22446a617bff56c1754e666f0bbb41815"}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(16, activation='relu', input_shape=(3,)))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n\nmodel.compile(optimizer='SGD', loss='squared_hinge', metrics=['accuracy'])","metadata":{"_uuid":"6d43c955e2dac981b7858021de173ceb8b09e461","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train, batch_size=10, epochs=20, verbose=1, validation_data=(X_test, y_test))","metadata":{"_uuid":"8d75707cb8c906834c485c805979fe3836a6ad6e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[test_loss, test_acc] = model.evaluate(X_test, y_test)\nprint(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))","metadata":{"_uuid":"95252c11f03124f97ffeb951dd1835aa7e29ee22","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the above model performs better but it also has lot of noise (loss) which can be neglected for prediction and use it for furthur prediction.\n\nThe above model is saved for furthur prediction.","metadata":{"_uuid":"1f462dff246920441300275da5591b6cf3536e03"}},{"cell_type":"code","source":"model.save('earthquake.h5')","metadata":{"_uuid":"56147c5973b579388f146c2da81e721ad4724b03","trusted":true},"execution_count":null,"outputs":[]}]}